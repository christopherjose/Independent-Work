{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# show all outputs of a cell (such as if df.head() and df.tail() are in the same cell\n",
    "#default is 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "o = open('/Users/Work/Desktop/Work/Projects/Springboard/Final_Files/reviews_pickle.pkl','r')\n",
    "df=pickle.load(o)\n",
    "df=pd.DataFrame(df)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing (NLP)** is the process of transforming natural-language into machine usable data.<br>\n",
    "A Bag-of-Words model consists of the steps: Tokenization, Transformation, Vectorization <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** is the splitting of the text into parts, either by words, characters, pairs, groups of words, etc.  <br>\n",
    "\n",
    "n-grams are groups of words in a split.<br>\n",
    "Unigrams, bigrams, and trigrams are on one-word, two-word, and three-word groups, respectively. <br>\n",
    "Tokens is what we call unigrams. <br>\n",
    "The corpus is the set of texts used for analysis.<br>\n",
    "Start with unigrams and expand to n-grams, if necessary.\n",
    "\n",
    "**Transformation** is the process of transforming tokens in best preparing them for the model.<br>  Examples include: *stemming*, meaning stripping words of their suffixes so that _jumping, jump, jumped_, etc can be in the same category, and making all letters lowercase<br>\n",
    " \n",
    "After tokenizing and transforming, make a **dictionary** of unique words (when dealing with just unigrams) with keys as words and values as occurrences.  Usually not all words are used to build the model, just the top-N words with the most number of occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization** is the process of creating a matrix of the dictionary keys (words/tokens/n-grams/features) as columns and observations as rows.  For a given row, each cell is the number of times that dictionary key appeared in that observation's text.  <br>\n",
    "\n",
    "**Stop words** are unimportant, filler words like \"the\", \"is\", etc that typically have high counts.  Most NLP libraries include pre-built stop word lists.  If your project has its own specific stop words that are not in these lists, select a stop-word threshold, such that any words that appear in over x% of the documents are excluded.  x is typically 90%. <br>\n",
    "\n",
    "Bag-of-words features (the transformed n-grams we selected that are dictionary keys) are called **sparse** because they have a lot of zero values across observations.  Only a small number of text features are typically found in a given text. <br>\n",
    "\n",
    "Recommended algorithms include those that can handle sparse data (Naive Bayes) or those than can handle many low-significance features (Random Forests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y=df.stars\n",
    "f= lambda x: 1 if x>=3 else 0\n",
    "#f= lambda x: 2 if x>=4 else 1 if x>=3 else 0 #if we wanted 3 categories\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split\\\n",
    "(df.text, Y, test_size=0.30, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>01pm</th>\n",
       "      <th>02</th>\n",
       "      <th>02pm</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>étage</th>\n",
       "      <th>étoile</th>\n",
       "      <th>étudiants</th>\n",
       "      <th>étudier</th>\n",
       "      <th>été</th>\n",
       "      <th>évitez</th>\n",
       "      <th>être</th>\n",
       "      <th>ö_ö</th>\n",
       "      <th>über</th>\n",
       "      <th>überholt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  001  00am  00pm  01  01pm  02  02pm  03    ...     étage  étoile  \\\n",
       "0   0    0    0     0     0   0     0   0     0   0    ...         0       0   \n",
       "1   0    0    0     0     0   0     0   0     0   0    ...         0       0   \n",
       "2   0    0    0     0     0   0     0   0     0   0    ...         0       0   \n",
       "3   0    0    0     0     0   0     0   0     0   0    ...         0       0   \n",
       "4   0    0    0     0     0   0     0   0     0   0    ...         0       0   \n",
       "\n",
       "   étudiants  étudier  été  évitez  être  ö_ö  über  überholt  \n",
       "0          0        0    0       0     0    0     0         0  \n",
       "1          0        0    0       0     0    0     0         0  \n",
       "2          0        0    0       0     0    0     0         0  \n",
       "3          0        0    0       0     0    0     0         0  \n",
       "4          0        0    0       0     0    0     0         0  \n",
       "\n",
       "[5 rows x 13572 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True,token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b') \n",
    "vectorizer.fit(X_train)\n",
    "X_train_count=vectorizer.transform(X_train)\n",
    "pd.DataFrame(X_train_count.toarray(),columns=vectorizer.get_feature_names()).head() #create df of term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit()** learns the vocabulary dictionary of all tokens in the documents.  We fit on the train split.  We could do transformer=vectorizer.fit(), then transformer.transform(), but vectorizer has an internal state so it remembers the dictionary vocab values after vectorizer.fit, so we can just do: vectorizer.fit(), then vectorizer.transform(). <br>\n",
    "**transform()** transforms documents to document-term matrix (count matrix) using only the vocabulary learned with fit(), which is just from the train split. <br>\n",
    "**fit_transform()** Performs fit() and then transform() with one function! <br>\n",
    "**stop_words** - the default is None, meaning no words will be excluded <br>\n",
    "**(lowercase=True)** - Default value of stop_words is None, all letters are converted to lowercase by default  <br>\n",
    "**token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'** - Default value is such that punctuation around words is ignored, so 'hi!' is the same as 'hi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer()** object's **fit_transform()** method learns the dictionary of words, and then uses the words in that dictionary to create a matrix of counts for all words in the column/list/array of data that is passed to it.  The argument **stop_words**='english' removes all words in the built-in stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5935, 13572)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape # rows are observations, columns are tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the CountVectorizer() object has been fitted to the data, it creates a dictionary of words as keys and counts as values.  This dictionary can be accessed using the **vocabulary** method of CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fawn', 4621),\n",
       " (u'raining', 9554),\n",
       " (u'writings', 13424),\n",
       " (u'gag', 5149),\n",
       " (u'hendertucky', 5792)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.items()[:5] #items() creates a list of tuples from the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'hi' and 'hello' appear 6884 and 6841 times, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5815"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('hi') \n",
    "vectorizer.vocabulary_.get('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stop-word list, as shown below, excludings 'hi' and 'hello'.  _True_ indicates that errors are returned, since the words cannot be found).  These words happen to be in this dataset, and I would like to add them to the stop-word list (to explore how this process would work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'show', 'anyway', 'fifty', 'four']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_list=list(set(text.ENGLISH_STOP_WORDS))\n",
    "print stop_list[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    (stop_list.index('hi')) \n",
    "except ValueError:\n",
    "    True\n",
    "try:\n",
    "    (stop_list.index('hello')) \n",
    "except ValueError:\n",
    "    True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_list=text.ENGLISH_STOP_WORDS.union(['hi','hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice to set a stop word threshold of .90 so that all words that appear in over 90% of the documents are excluded from dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_list,max_df=.90) #default value of max_df is 1.0\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "vectorizer.vocabulary_.get('hi')==None #'hi' now appears 0 times\n",
    "vectorizer.vocabulary_.get('hello')==None #'hello' now appears 0 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Final Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y=df.stars\n",
    "f= lambda x: 1 if x>=3 else 0\n",
    "#f= lambda x: 2 if x>=4 else 1 if x>=3 else 0 #if we wanted 3 categories\n",
    "Y=Y.map(f)\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split\\\n",
    "(df.text, Y, test_size=0.30, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "stop_list=list(set(text.ENGLISH_STOP_WORDS))\n",
    "vectorizer = CountVectorizer(stop_words=stop_list,max_df=.90) #default value of max_df is 1.0.  It's common to do .9.\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.model_selection\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(X_train_count, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF model reduces the impact of tokens that occur very frequently in a training corpus and are hence less informative (unlike the Bag-of-Words model).  It takes the matrix of counts from the Bag-of-Words model and transforms it into a matrix of TF-IDFs.  It does this by scaling down terms that are more frequent, and scaling up terms that are less frequent.  Words that had to be excluded in Bag-of-Words (stop-words) receive very low weights in this model, so this model accounts for stop-words without excluding them.  We can still choose to exclude them in this model though.  <br>\n",
    "\n",
    "TF-IDF = tf x idf <br>\n",
    "tf is term frequency <br>\n",
    "idf is inverse document frequency <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is the number of times the word appears, scaled by how infrequent it is in the document where the more infrequent it is, then the greater the scalar.\n",
    "If a rare word appears 1 time in a document, it will have a much higher tf-idf than an extremely common word that appears 1 time in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *TfidfTransformer*, the default formula is tf-idf= (tf x idf) where tf=count of terms. <br>\n",
    "By default, **sublinear_tf=False**. When True, tf is replaced with 1+log(tf), meaning sublinear tf scaling is used.  This accounts for the unreasonable assumption that 20 occurrences of a term in a document carries twenty times the significance of a single occurrence, which is what is implied by simply using a count.  Logging the count is one solution to this unreasonable assumption.   <br>\n",
    "\n",
    "\n",
    "By default, **smooth_idf=True**, which adds a 1 inside the log term: <br>\n",
    "**idf = log[(1+n)/(1+df)] + 1** <br>\n",
    "**=log((1+docs)/(1+docs with term)) + 1** <br>\n",
    "\n",
    "n is the total number of documents (observations) <br>\n",
    "df is the document frequency, the number of documents that contain the term t <br>\n",
    "\n",
    "The 1 within the log term prevents the possibility of zero division, meaning a term appearing in none of the documents.  The 1 outside the log term causes terms that appear in all documents to not be ignored.  Without the 1 added, idf=log(n/n)=0 and then (tf)(idf) = (tf)(0) = 0.  With the 1, idf=0+1=1 and then (tf)(1) = tf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **smooth_idf=False**, the 1 is removed from within the log term<br>\n",
    "**idf = log(n/df) +1** <br>\n",
    "**=log(docs / docs with term) + 1** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=df.stars\n",
    "f= lambda x: 1 if x>=3 else 0\n",
    "#f= lambda x: 2 if x>=4 else 1 if x>=3 else 0 #if we wanted 3 categories\n",
    "Y=Y.map(f)\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split\\\n",
    "(df.text, Y, test_size=0.30, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building the count matrices\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english',max_df=.90) \n",
    "X_train_count = vectorizer.fit_transform(X_train) \n",
    "X_test_count = vectorizer.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building the tf-idf matrices\n",
    "transformer=TfidfTransformer(smooth_idf=True, sublinear_tf=False) #transformer.idf_==None returns true at this point\n",
    "X_train_tfidf=transformer.fit_transform(X_train_count) #fit() and transform().  transformer.idf_ now returns idfs\n",
    "X_test_tfidf=transformer.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transformer.fit(X_train_count)** learns the idf vector from the train split, where the idf vector is the weights applied to the term freqencies in the tf-idf model.\n",
    "TfidfTransformer() has an internal state, so when you do TfidfTransformer().fit(X\\_train\\_count), idfs are generated from the term frequencies, and then when you call TfidfTransformer() again and access its idf\\_ attribute, you'll get the generated idfs. <br>\n",
    "**transformer.idf\\_** returns idfs from having fit the transformer on the train split<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.75696515,  8.5901785 ,  8.9956436 , ...,  8.9956436 ,\n",
       "        8.9956436 ,  8.9956436 ])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.toarray()[0] #array of tf-idf's for the first document\n",
    "#contains an array of TF-IDFs for each document, where each array has the TF-IDF for each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.783\n",
      "Precision score: 0.762\n",
      "Sensitivity score: 0.985\n",
      "F1 score: 0.859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
    "Y_pred=clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,  f1_score\n",
    "print 'Accuracy score: %r' %round((accuracy_score(Y_test, Y_pred)),3)\n",
    "print 'Precision score: %r' %round((precision_score(Y_test, Y_pred)),3)\n",
    "print 'Sensitivity score: %r' %round((recall_score(Y_test, Y_pred)),3)\n",
    "print 'F1 score: %r' %round((f1_score(Y_test, Y_pred)),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts a collection of raw documents into a matrix of TF-IDF features.  Combines CountVectorizer and TfidfTransformer into a single model, so that you can use the original data as an input (unlike TfidfTransformer, which requires the matrix of counts that is outputted from CountVectorizer())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=df.stars\n",
    "f= lambda x: 1 if x>=3 else 0\n",
    "#f= lambda x: 2 if x>=4 else 1 if x>=3 else 0 #if we wanted 3 categories\n",
    "Y=Y.map(f)\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split\\\n",
    "(df.text, Y, test_size=0.30, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=.95, smooth_idf=True,sublinear_tf=False,stop_words='english')\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.783\n",
      "Precision score: 0.762\n",
      "Sensitivity score: 0.985\n",
      "F1 score: 0.859\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
    "Y_pred=clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,  f1_score\n",
    "print 'Accuracy score: %r' %round((accuracy_score(Y_test, Y_pred)),3)\n",
    "print 'Precision score: %r' %round((precision_score(Y_test, Y_pred)),3)\n",
    "print 'Sensitivity score: %r' %round((recall_score(Y_test, Y_pred)),3)\n",
    "print 'F1 score: %r' %round((f1_score(Y_test, Y_pred)),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Naive Bayes Model by starting with Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem allows us to calculate the probability of an event occurring, given that another event has occurred, called the **posterior probability**, the probability that we are unaware of, from probabilities related to the event called **prior probabilities**, probabilities that we are aware of, that are given to us, and that do not reflect the fact that this other event has occurred.  These probabilities are simply just referred to as priors and posteriors.<br>  \n",
    "\n",
    "Another definition: The probability of an event when we have knowledge of conditions that might be related to the event.  https://en.wikipedia.org/wiki/Bayes'_theorem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Bayes Theorem formula - my favorite formula (more formulas shown below):\n",
    "                    P(Likelihood of Evidence) * Prior prob of outcome\n",
    "P(outcome|evidence) = _________________________________________________\n",
    "                                         P(Evidence)                                        \n",
    "'''   ;                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the probabiltiy that a sample is actually positive, given a positive prediction for that sample. <br>\n",
    "P = Positive Sample (Y=1)<br>\n",
    "N = Negative Sample (Y=0) <br>\n",
    "TP = True Positive = positive prediction is correct (Y_pred=1 and Y=1)<br>\n",
    "Pos = Positive Prediction (Y_pred=1)<br>\n",
    "Sensitivity is the proportion of positives that are predicted as positive.  <br>\n",
    "Specificity is the proportion of negatives that are predicted as negative.  <br>\n",
    "1-Specificity is the proportion of negatives that are predicted as positive. <br>\n",
    "<font size=+1>$Posterior\\_Probability = \\frac{Conditional\\_ProbabilityxPrior\\_Probability}{Evidence(=Prior\\_Probability)} $ <br>                                       \n",
    "<font size=+1>$P(P | Pos) = \\frac{P(Pos|P) P(P)}{P(Pos)} $\n",
    "<br>\n",
    "$P(Y=1 | Y\\_pred=1) = \\frac{P(Y\\_pred=1|Y=1) P(Y=1)}{P(Y\\_pred=1)} $ <br>\n",
    "<br>$= \\frac{P(TP)P(P)}{P(TP)P(P) + P(FP) P(N)} $<br>\n",
    "<br>$= \\frac{Sensitivity P(P)}{SensitivityP(P) + (1 - Specificity) P(N)} $<br>\n",
    "\n",
    "\n",
    "<font size=+1>$P(Pos)$<br>\n",
    "$= P(Pos|P)P(P)+P(Pos|N)P(N)$<br>\n",
    "$= P(TP)P(P)+P(FP)P(N)$<br>\n",
    "$= SensitivityP(P) + (1 - Specificity) P(N) $ <br>\n",
    "\n",
    "$P(P)+P(N)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem says that the likelihood of an actual positive given a positive prediction equals: <br> (the likelihood of a positive prediction, given an actual positive) x (the likelihood of an actual positive) / (the likelihood of a positive prediction) <br>\n",
    "\n",
    "P(Pos|P) is the probablity of a positive prediction given the sample is actually positive.  <br>This is the same thing as the probability of a true positive P(TP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem applied to the Starbucks text review TF-IDF model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split statistics\n",
      "The posterior probability of Y=1, given Y-hat=1:      0.848\n",
      "The prior probability of Y-hat=1:                     0.783\n",
      "Accuracy (same as above, but calculated differently)  0.783\n",
      "The prior probability of Y=1:                         0.674\n",
      "Sensitivity                                           0.985\n",
      "Specificity                                           0.635\n"
     ]
    }
   ],
   "source": [
    "predictions=pd.DataFrame(zip(Y_pred,Y_test),columns=['Y-hat','Y'])\n",
    "total=len(Y_test)\n",
    "pos=sum(Y_pred)\n",
    "p=sum(Y_test)\n",
    "n=len(list(itertools.ifilter(lambda x: x==0, Y_test)))\n",
    "import itertools\n",
    "tpfp=list(itertools.compress(Y_pred,Y_test))\n",
    "tnfn=list(itertools.compress(Y_pred,Y_test==False))\n",
    "tp=sum(tpfp) #return values of Y_pred only where values of Y_test are true (=1)\n",
    "fp=len(list(itertools.ifilter(lambda x: x==1, tnfn))) #filters and returns values where x==1\n",
    "fn=len(list(itertools.ifilter(lambda x: x==0, tpfp))) #filters and returns values where x==1\n",
    "tn=len(list(itertools.ifilter(lambda x: x==0, tnfn))) #filters and returns values where x==0\n",
    "sensitivity=tp/float(p)\n",
    "specificity=fp/float(n)\n",
    "prob_p=p/float(total)\n",
    "prob_n=n/float(total)\n",
    "prob_pos=sensitivity*prob_p+(1-specificity)*prob_n\n",
    "posterior=sensitivity*prob_p/prob_pos\n",
    "print 'Test split statistics'\n",
    "print 'The posterior probability of Y=1, given Y-hat=1:      %r' %(round(posterior,3))\n",
    "print 'The prior probability of Y-hat=1:                     %r' %(round(prob_pos,3))\n",
    "print 'Accuracy (same as above, but calculated differently)  %r' %(round((tn+tp)/float(total),3))\n",
    "print 'The prior probability of Y=1:                         %r' %(round(prob_p,3))\n",
    "print 'Sensitivity                                           %r' %(round(sensitivity,3))\n",
    "print 'Specificity                                           %r' %(round(specificity,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 527, 26, 1688)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 303,  527],\n",
       "       [  26, 1688]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.where(predictions.Y==0,'yes',0)\n",
    "#C = np.where(cond, A, B)\n",
    "#defines C to be equal to A where cond is True, and B where cond is False.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to predict the probability of a categorical target taking on a certain value, assuming the values of multiple features, then we can use the Naive Bayes model.  This model is built using Bayes Theorem, but assuming that features are **conditionally independent**.  It does not assume that features are **independent**, and independence of features does not necessarilly imply conditional independence of features.  This conditional independence assumption, a simplification, is why we call this model a \"Naive\" Bayes model.  This makes the formula simpler to calculate, which is good because usually there are multiple pieces of evidence.  While this assumption often DOES NOT hold, this model is still known for outperforming many other more sophisticated models!   <br>\n",
    "\n",
    "1. This model estimates prior and conditional probabilities from the training data: P(feature j=c), P(target y=k).  \n",
    "2. It calculates the probability of the target taking value k, given the evidence of the observation (certain feature values) by using Bayes Theorem and assuming features are conditionally independent.  \n",
    "3. It selects the target value that producest the maximum conditional probability.  \n",
    "\n",
    "Thus, for a given observation, this model predicts a target value that has the maximium conditional probability of occurring, given the evidence (feature values) for that observation. \n",
    "\n",
    "Thus,\n",
    "<font size=+2>$\\frac{P(y|  x1,x2,...,xj) = P(x1|y)P(x2|y)...P(xj|y)P(y)}{P(x1,x2,...,xj)}$ <font size=+0><br>\n",
    "where y is the target, categorical variable equaling a value and x1,...,xj are the independent features.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier <br>\n",
    "https://en.wikipedia.org/wiki/Conditional_independence<br>\n",
    "http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification<br>\n",
    "http://www.saedsayad.com/naive_bayesian.htm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **chain rule** in probability theory allows joint probabilities to be calculated using conditional probabilties:<br>  P(x1,x2,x3)=P(x3|x2,x1)P(x2,x1) <br>\n",
    "=P(x3|x2,x1)P(x2|x1)P(x1) <br>\n",
    "\n",
    "The **conditional independence** assumption is that the knowledge/occurrence of x1 (x2) does not affect the likelihood of x2 (x1), given that Y has occurred.   Conditional independence does not imply independence.  Independence does not imply conditional independence.  (source: https://en.wikipedia.org/wiki/Conditional_independence)<br>\n",
    "P(x2|x1,Y) = P(x2|Y) <br>\n",
    "Equivalently and invoking the chain rule, <br>\n",
    "P(x2,x1|Y) = P(x2|Y)P(x1|Y)\n",
    "\n",
    "In estimating the likelihood of outcome Y given the evidence (feature values) for an observation,\n",
    "P(x1,x2|Y)=P(x1,x2,Y)/P(Y) <br>\n",
    "=P(x2|x1,Y)P(x1|Y)_P(Y)**/**P(Y)_ by the chain rule (notice that the P(Y)'s cancel out) <br>\n",
    "=P(x2|Y)P(x1|Y) by the the conditional independence assumption <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' In other words:\n",
    "P(Outcome|Multiple Evidence) = \n",
    "P(Evidence1|Outcome) * P(Evidence2|outcome) * ... * P(EvidenceN|outcome) * P(prior prob of outcome)\n",
    "_________________________________________________________________________________________________\n",
    "\n",
    "                                 P(Multiple Evidence)\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is suitable for discrete (0,1,2,3) features and thus is great for word counts, though fractional counts, such as for TF-IDF may also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity's Machine Learning Engineer - Naive_Bayes_tutorial Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**source**: <br>\n",
    "https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb <br>\n",
    "**forum post where I mention error:**\n",
    "https://discussions.udacity.com/t/error-in-notation-naive-bayes-tutorial-ipynb-practice-project/230754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The question: what is the probability that Jill says the words freedom and immigration?** <br>\n",
    "The real question: what is the probability that the words freedom and immigration are said in a speech, given that the candidate is Jill?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)<br>\n",
    "Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)<br>\n",
    "Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)<br>\n",
    "Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)<br>\n",
    "Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)<br>\n",
    "Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G) <br>\n",
    "P(G) = .5 <br>\n",
    "P(J) = .5 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(J|F&I) = P(F&I&J) / P(F&I) <br>\n",
    "\n",
    "**As for the numerator:** <br>\n",
    "By the chain rule, a joint probability can be expressed using conditional probabilities: <br>\n",
    "= P(F|I&J)P(I&J) <br>\n",
    "= P(F|I&J)P(I|J)P(J) <br>\n",
    "Due to the independence assumption of features,P(F|I&J)=P(F|J) : <br>\n",
    "P(F&I&J)=P(F|J)P(I|J)P(J) <br>\n",
    "\n",
    "**As for the denominator:** <br>\n",
    "Due to the independence assumption of features: <br>\n",
    "P(F&I)=P(F&I&J)+P(F&I&G)<br>\n",
    "By the chain rule: <br>\n",
    "=P(F|I&J)P(I&J)+P(F|I&G)P(I&G)<br>\n",
    "=P(F|I&J)P(I|J)P(J)+P(F|I&G)P(I|G)P(G)<br>\n",
    "Due to the independence assumption of features: <br>\n",
    "P(F&I)=P(F|J)P(I|J)P(J)+P(F|G)P(I|G)P(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666668"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator=.1*.1*.5\n",
    "denominator=.1*.1*.5+.7*.2*.5\n",
    "numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
